{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efa08066-4289-4781-902c-7e83e244857d",
   "metadata": {},
   "source": [
    "# Subset ERA5 pressure level data\n",
    "\n",
    "This notebook provides a way to subset ERA5 pressure level analysis by conserving its vertically integrated quantities.\n",
    "\n",
    "The subsetting approach here is computationally heavy. Thus, Dask workers are applied and handled by PBS job sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70332162-6c47-4172-a003-326059f10045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import dask\n",
    "import zarr\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from glob import glob\n",
    "\n",
    "sys.path.insert(0, os.path.realpath('../libs/'))\n",
    "import verif_utils as vu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57c0e5c6-9475-4dc0-bdde-9e07a8ab4536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from dask.distributed import Client\n",
    "from dask_jobqueue import PBSCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa740e32-0024-4dec-b2c9-ce7c7f92f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0068fbd7-18c4-496a-a1f7-c63f26c59544",
   "metadata": {},
   "source": [
    "## Mass conserved vertical level subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a190028-666a-4e0f-b8bc-a1a402fd2e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------- #\n",
    "# subsetting function\n",
    "def integral_conserved_subset_all_vars(ds, level_p, ind_select):\n",
    "    # Precompute the level differences and midpoints\n",
    "    diff_level_p = np.diff(level_p)\n",
    "    \n",
    "    # Create a helper function to compute the integral for each column\n",
    "    def integral_conserved_subset_1d(x_column):\n",
    "        x_column_midpoint = 0.5 * (x_column[1:] + x_column[:-1])\n",
    "        x_column_area = x_column_midpoint * diff_level_p\n",
    "        \n",
    "        # Allocate the output array\n",
    "        out_column_a = np.empty(len(ind_select)-1)\n",
    "        out_column_a.fill(np.nan)\n",
    "        \n",
    "        for i_ind, ind in enumerate(ind_select[:-1]):\n",
    "            ind_start = ind\n",
    "            ind_end = ind_select[i_ind+1]\n",
    "            out_column_a[i_ind] = np.sum(x_column_area[ind_start:ind_end]) / (level_p[ind_end] - level_p[ind_start])\n",
    "        \n",
    "        return out_column_a\n",
    "    \n",
    "    # Apply the function along the 'level' dimension and specify output_sizes in dask_gufunc_kwargs\n",
    "    ds_out = xr.apply_ufunc(\n",
    "        integral_conserved_subset_1d, ds,\n",
    "        input_core_dims=[['level']],\n",
    "        output_core_dims=[['new_level']],\n",
    "        vectorize=True,  # Broadcast across other dimensions\n",
    "        dask='parallelized',  # Enable Dask parallelism if ds is chunked\n",
    "        dask_gufunc_kwargs={\n",
    "            'allow_rechunk': True,  # Allow rechunking if necessary\n",
    "            'output_sizes': {'new_level': len(ind_select)-1}  # Specify the size of the new dimension\n",
    "        },\n",
    "        output_dtypes=[float]\n",
    "    )\n",
    "    \n",
    "    return ds_out\n",
    "# --------------------------------------------------------------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77331573-6b69-4a51-83ca-81c25b57654c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = os.path.realpath('data_config_6h.yml')\n",
    "\n",
    "with open(config_name, 'r') as stream:\n",
    "    conf = yaml.safe_load(stream)\n",
    "\n",
    "# save to zarr\n",
    "base_dir = conf['zarr_opt']['save_loc'] + 'upper_subset/' \n",
    "if not os.path.exists(base_dir):\n",
    "    os.makedirs(base_dir)\n",
    "\n",
    "level_p_select = np.array(conf['zarr_opt']['subset_level'])\n",
    "level_midpoints = 0.5 * (level_p_select[1:] + level_p_select[:-1])\n",
    "\n",
    "compress = zarr.Blosc(cname='zstd', clevel=1, shuffle=zarr.Blosc.SHUFFLE, blocksize=0)\n",
    "\n",
    "\n",
    "chunk_size = conf['zarr_opt']['chunk_size_4d']\n",
    "\n",
    "chunk_size_4d = dict(chunks=(chunk_size['time'],\n",
    "                             chunk_size['level'],\n",
    "                             chunk_size['latitude'],\n",
    "                             chunk_size['longitude']))\n",
    "dict_encoding = {}\n",
    "\n",
    "for i_var, var in enumerate(conf['RDA']['varname_upper_air']):\n",
    "    dict_encoding[var] = {'compressor': compress, **chunk_size_4d}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6d2288a-da06-4525-b54f-406ea53e56c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2000\n",
    "\n",
    "load_name = conf['RDA']['save_loc'] + 'upper_air/' + conf['RDA']['prefix'] + '_upper_air_{}.zarr'.format(year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3050f9e-ba3a-4233-bb5a-94240593c476",
   "metadata": {},
   "source": [
    "**Raw version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5b45c50-8bf6-4fb0-8912-32ac6c399c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_plevel = xr.open_zarr(load_name, consolidated=True)\n",
    "# level_p = np.array(ds_plevel['level'])\n",
    "# mask = np.isin(level_p, level_p_select)\n",
    "# ind_select = np.where(mask)[0]\n",
    "\n",
    "# ds_plevel = ds_plevel.chunk({'level': -1})\n",
    "# ds_plevel_subset = integral_conserved_subset_all_vars(ds_plevel, level_p, ind_select)\n",
    "# ds_plevel_subset = ds_plevel_subset.assign_coords(new_level=level_midpoints)\n",
    "# ds_plevel_subset = ds_plevel_subset.rename({'new_level': 'level'})\n",
    "# ds_plevel_subset = ds_plevel_subset.transpose('time', 'level', 'latitude', 'longitude')\n",
    "# ds_plevel_subset = ds_plevel_subset.chunk(chunk_size)\n",
    "\n",
    "# save_name = base_dir + conf['zarr_opt']['prefix'] + '_upper_air_{}.zarr'.format(year)\n",
    "# # ds_plevel_subset.to_zarr(save_name, mode=\"w\", consolidated=True, compute=True, encoding=dict_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4421be-ebca-4102-8973-e4814a1673bf",
   "metadata": {},
   "source": [
    "**Distributed on Dask Client** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1df5277-3ac9-4b3c-a532-0906062ba254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask client does not exist yet.\n",
      "Cluster dashboard: http://128.117.208.150:8787/status\n"
     ]
    }
   ],
   "source": [
    "# Check if Dask client exists and shut it down\n",
    "if 'client' in locals() and isinstance(client, Client):\n",
    "    client.shutdown()\n",
    "    print('...shutdown existing Dask client...')\n",
    "else:\n",
    "    print('Dask client does not exist, bulilding ...')\n",
    "\n",
    "# Set up the Dask cluster\n",
    "project_num = 'NAML0001'\n",
    "\n",
    "cluster = PBSCluster(\n",
    "    account=project_num,\n",
    "    walltime='12:00:00',\n",
    "    cores=1,\n",
    "    memory='70GB',\n",
    "    shared_temp_directory='/glade/derecho/scratch/ksha/tmp/',\n",
    "    queue='casper'\n",
    ")\n",
    "cluster.scale(jobs=40)\n",
    "client = Client(cluster)\n",
    "\n",
    "print(f\"Cluster dashboard: {client.dashboard_link}\")\n",
    "\n",
    "ds_plevel = xr.open_zarr(load_name, consolidated=True)\n",
    "level_p = np.array(ds_plevel['level'])\n",
    "mask = np.isin(level_p, level_p_select)\n",
    "ind_select = np.where(mask)[0]\n",
    "\n",
    "ds_plevel = ds_plevel.chunk({'level': -1})\n",
    "ds_plevel_subset = integral_conserved_subset_all_vars(ds_plevel, level_p, ind_select)\n",
    "ds_plevel_subset = ds_plevel_subset.assign_coords(new_level=level_midpoints)\n",
    "ds_plevel_subset = ds_plevel_subset.rename({'new_level': 'level'})\n",
    "ds_plevel_subset = ds_plevel_subset.transpose('time', 'level', 'latitude', 'longitude')\n",
    "ds_plevel_subset = ds_plevel_subset.chunk(chunk_size)\n",
    "\n",
    "save_name = base_dir + conf['zarr_opt']['prefix'] + '_upper_air_{}.zarr'.format(year)\n",
    "ds_plevel_subset.to_zarr(save_name, mode=\"w\", consolidated=True, compute=True, encoding=dict_encoding)\n",
    "\n",
    "# Shutdown Dask cluster and client\n",
    "print('...shutting down Dask client and cluster...')\n",
    "client.close()\n",
    "cluster.close()\n",
    "\n",
    "# Removing Dask worker files\n",
    "print('...removing Dask worker files...')\n",
    "fns_rm = sorted(glob('./dask-worker*'))\n",
    "print(f\"Found {len(fns_rm)} Dask worker files.\")\n",
    "for fn in fns_rm:\n",
    "    if os.path.exists(fn):\n",
    "        os.remove(fn)\n",
    "        print(f\"Removed: {fn}\")\n",
    "    else:\n",
    "        print(f\"File not found: {fn}\")\n",
    "\n",
    "print('...all done...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a093f9fb-848c-44be-acff-4df9a43de7d1",
   "metadata": {},
   "source": [
    "### How to run above as PBS jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fa1671-f071-4443-bdc0-06ee726a40c5",
   "metadata": {},
   "source": [
    "One year at a time. Start from year 1979, when it finishes, move to 1980, etc.\n",
    "\n",
    "PBS script name: `ERA5_PP_plevel_subset_6h_${year}.sh`\n",
    "\n",
    "Inside the PBS script, the python code above, including the dask client component, is called.\n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "\n",
    "# List of years to process\n",
    "start_year=1979\n",
    "end_year=2023\n",
    "\n",
    "# Initialize a variable to hold the job ID of the previous job\n",
    "prev_job_id=\"\"\n",
    "\n",
    "# Loop through each year\n",
    "for year in $(seq $start_year $end_year); do\n",
    "    job_script=\"ERA5_PP_plevel_subset_6h_${year}.sh\"\n",
    "\n",
    "    if [[ -z \"$prev_job_id\" ]]; then\n",
    "        # Submit the first job without any dependency\n",
    "        job_id=$(qsub $job_script)\n",
    "    else\n",
    "        # Submit the current job with a dependency on the previous job\n",
    "        job_id=$(qsub -W depend=afterok:$prev_job_id $job_script)\n",
    "    fi\n",
    "\n",
    "    # Extract only the job ID from the qsub output\n",
    "    prev_job_id=$(echo $job_id | cut -d '.' -f 1)\n",
    "\n",
    "    echo \"Submitted job for year $year with job ID $prev_job_id\"\n",
    "done\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3554444-7dbe-4ff5-9103-2450204a57d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
