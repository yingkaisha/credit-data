{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bc0c8f6-4b51-48fb-a4f4-693dd263c94e",
   "metadata": {},
   "source": [
    "# Gather IFS forecast files from WeatherBench 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36e1a1c5-9c82-4467-a19f-dd0a87f89716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "from glob import glob\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62a34ab8-fb3e-40ef-920e-8abe0ba8467c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------- #\n",
    "# interpolation utils\n",
    "# from scipy.interpolate import griddata\n",
    "import scipy.interpolate as spint\n",
    "from scipy.spatial import Delaunay\n",
    "import itertools\n",
    "\n",
    "def interp_weights(xy, uv, d=2):\n",
    "    tri = Delaunay(xy)\n",
    "    simplex = tri.find_simplex(uv)\n",
    "    vertices = np.take(tri.simplices, simplex, axis=0)\n",
    "    temp = np.take(tri.transform, simplex, axis=0)\n",
    "    delta = uv - temp[:, d]\n",
    "    bary = np.einsum('njk,nk->nj', temp[:, :d, :], delta)\n",
    "    return vertices, np.hstack((bary, 1 - bary.sum(axis=1, keepdims=True)))\n",
    "\n",
    "def interpolate(values, vtx, wts):\n",
    "    return np.einsum('nj,nj->n', np.take(values, vtx), wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84efd602-c645-4154-86ab-3175c4a4c0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.realpath('../libs/'))\n",
    "import verif_utils as vu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab164d57-2396-429d-b51c-6d3b47a40146",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = os.path.realpath('verif_config.yml')\n",
    "\n",
    "with open(config_name, 'r') as stream:\n",
    "    conf = yaml.safe_load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b122b0e6-9105-49af-ae16-4fabffa5cf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'IFS'\n",
    "filename_prefix = 'IFS_%Y-%m-%dT%HZ.nc'\n",
    "save_loc = conf[model_name]['save_loc'] + filename_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bd658ad-aa6b-495a-b656-01c1acb57110",
   "metadata": {},
   "outputs": [],
   "source": [
    "verif_ind_start = 1000; verif_ind_end = 4500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bf444d8-4ce9-4ed6-9d11-da69d5e490ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolation weights were computed for 90N -> 90S\n",
    "# IFS is 90S -> 90N, should be flipped\n",
    "flip_lat = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b52c0bcc-1cd3-44ab-813a-b86b97702d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import original IFS from WeatherBench GS\n",
    "ds_IFS = xr.open_zarr('gs://weatherbench2/datasets/hres/2016-2022-0012-1440x721.zarr')\n",
    "\n",
    "# --------------------------------------------------------- #\n",
    "# subset and organize their xr.Dataset\n",
    "\n",
    "variables_levels = conf[model_name]['verif_variables']\n",
    "ds_IFS = vu.ds_subset_everything(ds_IFS, variables_levels)\n",
    "\n",
    "# unify variable and coord names\n",
    "ds_IFS = ds_IFS.rename({'latitude':'lat','longitude':'lon'})\n",
    "ds_IFS = ds_IFS.rename(conf[model_name]['rename_variables'])\n",
    "ds_IFS = ds_IFS.squeeze('level')\n",
    "ds_IFS = ds_IFS.isel(prediction_timedelta=slice(1, 42)) # <--- IFS lead time begins at 0, we drop 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca8ccdb4-0af0-433c-83d1-4c088bad13d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------- #\n",
    "# preparing for the regriding and separated *.nc save \n",
    "\n",
    "# IFS lat/lons\n",
    "x_IFS = np.array(ds_IFS['lon'])\n",
    "y_IFS = np.array(ds_IFS['lat'])\n",
    "\n",
    "if flip_lat:\n",
    "    y_IFS = np.flipud(y_IFS)\n",
    "    \n",
    "lon_IFS, lat_IFS = np.meshgrid(x_IFS, y_IFS)\n",
    "\n",
    "# OUR lat/lons\n",
    "OURS_dataset = xr.open_dataset(conf['geo']['geo_file_nc'])\n",
    "x_OURS = np.array(OURS_dataset['longitude'])\n",
    "y_OURS = np.array(OURS_dataset['latitude'])\n",
    "lon_OURS, lat_OURS = np.meshgrid(x_OURS, y_OURS)\n",
    "shape_OURS = lon_OURS.shape\n",
    "\n",
    "# pick the years we need\n",
    "year_range = conf[model_name]['year_range']\n",
    "years_pick = np.arange(year_range[0], year_range[1]+1, 1)\n",
    "\n",
    "# get initialization time\n",
    "init_time = pd.to_datetime(ds_IFS['time'])\n",
    "# get forecast lead time\n",
    "N_leads = len(ds_IFS['prediction_timedelta'])\n",
    "# get variables\n",
    "list_var_names = list(ds_IFS.keys())\n",
    "\n",
    "# interp weights\n",
    "temp_data = np.load(conf['geo']['regrid_weights_numpy'], allow_pickle=True)[()]\n",
    "vtx = temp_data['vtx']\n",
    "wts = temp_data['wts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "035c41d6-9ebe-4060-b721-4ca8f300dc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing IFS_2018-12-24T00Z.nc\n",
      "Interpolate Z500\n",
      "Interpolate Q500\n",
      "Interpolate T500\n",
      "Interpolate U500\n",
      "Interpolate V500\n",
      "Interpolate SP\n",
      "Interpolate t2m\n",
      "Save to /glade/campaign/cisl/aiml/ksha/CREDIT/gathered/IFS/IFS_2018-12-24T00Z.nc\n"
     ]
    }
   ],
   "source": [
    "for i_dt, dt_index in enumerate(init_time[verif_ind_start:verif_ind_end]):\n",
    "\n",
    "    # indexing could start from nonzero\n",
    "    i_dt = i_dt + verif_ind_start\n",
    "    \n",
    "    # init year is within selection \n",
    "    if dt_index.year in years_pick:\n",
    "\n",
    "        # get file name\n",
    "        save_name = datetime.strftime(dt_index, save_loc)\n",
    "\n",
    "        # save and skip exists\n",
    "        if os.path.exists(save_name) is False:\n",
    "            \n",
    "            print('Processing {}'.format(os.path.basename(save_name)))\n",
    "            \n",
    "            # allocate regrided dataset\n",
    "            ds_IFS_regrid = xr.Dataset()\n",
    "            ds_IFS_regrid = ds_IFS_regrid.assign_coords({'lon': x_OURS, 'lat': y_OURS})\n",
    "            \n",
    "            # subset on initialization time\n",
    "            ds_IFS_slice = ds_IFS.isel(time=slice(i_dt, i_dt+1))\n",
    "            # -------------------------------------------------------------------------- #\n",
    "            # interpolation section\n",
    "    \n",
    "            # assign time and lead time coord info to the allocated xr.Dataset\n",
    "            ds_IFS_regrid['time'] = ds_IFS_slice['time']\n",
    "            ds_IFS_regrid['prediction_timedelta'] = ds_IFS_slice['prediction_timedelta']\n",
    "            \n",
    "            # loop through variables\n",
    "            for var_name in list_var_names:\n",
    "                \n",
    "                print('Interpolate {}'.format(var_name))\n",
    "                # allocate regridded IFS on multi-lead times\n",
    "                allocate_interp = np.empty((N_leads,)+shape_OURS)\n",
    "                \n",
    "                # loop through lead times\n",
    "                for i_lead in range(N_leads):\n",
    "    \n",
    "                    # select the variable on the current lead time\n",
    "                    IFS_var = ds_IFS_slice[var_name].isel(time=0, prediction_timedelta=i_lead)\n",
    "    \n",
    "                    # ========================================================================== #\n",
    "                    if flip_lat:\n",
    "                        IFS_var = np.flipud(IFS_var)\n",
    "                    # scipy.interpolate.griddata(method='linear') with manually inputted weights #\n",
    "                    IFS_var_regrid = interpolate(IFS_var, vtx, wts)\n",
    "                    IFS_var_regrid = np.reshape(IFS_var_regrid, shape_OURS)\n",
    "                    allocate_interp[i_lead, ...] = IFS_var_regrid\n",
    "                    # ========================================================================== #\n",
    "                \n",
    "                # np.array --> xr.DataArray\n",
    "                IFS_var_regrid_da = xr.DataArray(\n",
    "                    allocate_interp[None, ...], \n",
    "                    coords={\n",
    "                        'time': ds_IFS_slice['time'],\n",
    "                        'prediction_timedelta': ds_IFS_slice['prediction_timedelta'], \n",
    "                        'lat': y_OURS, \n",
    "                        'lon': x_OURS,},\n",
    "                    dims=['time', 'prediction_timedelta', 'lat', 'lon'])\n",
    "    \n",
    "                # add xr.DataArray to the allocated xr.Dataset\n",
    "                ds_IFS_regrid[var_name] = IFS_var_regrid_da\n",
    "    \n",
    "            # ============================================================================================== #\n",
    "            # --------------------------------- Unify the lead time coords --------------------------------- #\n",
    "            ## WeatherBench uses 'prediction_timedelta' for (relative) lead time and 'time' for init time\n",
    "            ## We use 'time' for forecasted time\n",
    "            ## this code block does the conversion\n",
    "            absolute_lead_time = np.array((dt_index + ds_IFS_regrid['prediction_timedelta'])) # time + delta\n",
    "            ds_IFS_regrid = ds_IFS_regrid.squeeze('time')\n",
    "            ds_IFS_regrid = ds_IFS_regrid.drop_vars('time')\n",
    "            ds_IFS_regrid = ds_IFS_regrid.drop_vars('level')\n",
    "            ds_IFS_regrid = ds_IFS_regrid.rename({'prediction_timedelta':'time'})\n",
    "            ds_IFS_regrid['time'] = absolute_lead_time\n",
    "            # ============================================================================================== #\n",
    "            # Save to netCDF4\n",
    "            ds_IFS_regrid.to_netcdf(save_name)\n",
    "            print('Save to {}'.format(save_name))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05132ad-0d48-4aca-ae4a-80facb90926e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
